\section{Distinct Elements}
Given, a stream of size $m$ containing numbers from $[n]$, we have to approximate the number of elements with non-zero frequency. To calculate the exact value the space required:

\begin{itemize}
\item $O(n)$ bits. (maintain a vector of length n).
\item $O(m \log (n))$ bits. (save m numbers, each taking $log(n)$ bits).
\end{itemize}

Since, this complexity is not feasible as $m$,$n$ can be very large, we'll look at algorithm for approximating the distinct count value.

\subsubsection{Hash Function}
\begin{itemize}
\item $h : [n] \rightarrow [0,1]$
\item $h(i)$ is uniformly distributed in $[0,1]$.
\end{itemize}

\subsection{Algorithm [Flajolet-Martin 1985]}
We maintain a variable $z$.
\begin{enumerate}
\item Initialize $z = 1$.
\item Whenever $i$ is encountered: $z = \min{(z,h(i))}$
\item When done, output $1/z -1$.
\end{enumerate}

Now, we'll prove the algorithm works in a similar fashion followed in previous lecture.
Let $d$ be number of distinct elements.

\begin{claim}
$E[z] = d+1$
\end{claim}

\begin{proof}
$z$ is the minimum of $d$ random numbers in $[0,1]$. Pick another random number $a \in [0,1]$. The probability $a<z$:
\begin{enumerate}
\item exactly z
\item probability it's smallest among $d+1$ reals : $1/(d+1)$
\end{enumerate}
Equating these two, one can prove the claim.
\end{proof}

\begin{claim}
$\text{var}[z] \leq 2/d^{2}$
\end{claim}

\begin{proof}
It can be done in a similar fashion described in previous lecture.
\end{proof}

\subsubsection{$(1+\eps)$ approximation Algorithm }
We can take $Z = (z_{1} + z_{2} + ... z_{k})/k$ for independent $z_{1}, ... z_{k}$

\subsection{Alternate Algorithm: Bottom-k}
Instead of just use the minimum value of hash function for $i$ inputs, we'll maintain the $k$ smallest hashes seen.
\begin{enumerate}
\item Initialize $(z_{1}, z_{2},...z_{k}) = 1$.
\item Keep $k$ smallest hashes seen, s.t. $z_{1}\leq z_{2}\leq...z_{k}$
\item When done, output $\hat{d} = k/z_{k}$
\end{enumerate}

\begin{claim}
The following claims are stated:
\begin{itemize}
\item $Pr[\hat{d} > (1 + \eps)d] \leq 0.05$
\item $Pr[\hat{d} < (1 - \eps)d] \leq 0.05$
\item Overall probability that $\hat{d}$ outside range is at most 0.1
\end{itemize}
\end{claim}

\begin{proof}
To compute $Pr[\hat{d} > (1+\eps)d]$:
\begin{itemize}
\item Define $X_{i} = 1$ iff $h(i) < \dfrac{k}{(1+\eps)d}$
\item Then $\hat{d} > (1+\eps)d$ iff $\Sigma_{i} X_{i} > k$
\item if $\Sigma_{i} X_{i} > k$\\
  $\iff \exists$ at least $k$ numbers for which $h(i) < \dfrac{k}{(1+\eps)d}$\\
    \begin{equation}
      \iff z_{k} < \dfrac{k}{(1+\eps)d}
      \iff \dfrac{k}{z_{k}} > (1+\eps)d
      \iff \hat{d} > (1+\eps)d
    \end{equation}
\item
  $E[X_{i}] = \dfrac{k}{(1+\eps)d}$\\
  $E[\Sigma_{i} X_{i}] = d E[X_{i}] = \dfrac{k}{1 + \eps}$\\
  $\text{var}[\Sigma_{i} X_{i}] = d \text{var}[X_{i}] \leq dE[X_{1}^{2}] \leq  \dfrac{k}{1+\eps} \leq k$\\
  (Since $X_{1} \in \{0,1\}$, $E[X_{1}^{2}] = E[X_{i}]$)
\item By Chebyshev:
    $Pr[|\Sigma X_{i} - \dfrac{k}{1+\eps}| > \sqrt{20k}] \leq 0.05 \implies Pr[\Sigma X_{i} > \dfrac{k}{1+\eps} + \sqrt{20k}] \leq 0.05 $\\
    \begin{itemize}
    \item
      (For $\eps < 1/2$ and $k=c/\eps^{2}$)\\
      $\dfrac{k}{1+\eps} + \sqrt{20k} \leq k(1-\eps+\eps^{2}) + \sqrt{20k}$ (Taylor Series Expansion)\\
      $ \leq k - k\eps/2 + 5\sqrt{c}/\eps$
      $ = k - c / 2\eps + 5\sqrt{c}/\eps$\\
      $ < k $ where $c > 100$
    \item
      Since $k > \dfrac{k}{1+\eps} + \sqrt{20k} $ in our case and $\Sigma X_{i}$ is monotonically increasing, $Pr[\Sigma X_{i} > k] \leq Pr[\Sigma X_{i} > \dfrac{k}{1+\eps} + \sqrt{20k}] \leq 0.05$

    \end{itemize}
\end{itemize}
\end{proof}

\subsection{Hash functions in stream}
The hash function we used has two practical issues: (1) the return value should be a real number. (2) how do we store it?

Discretization can solve the first issue. Instead of all the real numbers in $[0, 1]$, we use hash function with range $\{0, \frac{1}{M}, \frac{2}{M}, \frac{3}{M}, \ldots, 1\}$. For large $M \gg n^{3}$, the probability that $d \le n$ random numbers collide is at most $\frac{1}{n}$.

For the second issue, we use pairwise independent function instead of independent function.

\begin{definition}
$h: [n] \rightarrow \{1, 2, \ldots M\}$ is pairwise independent if for all $i \ne j$ and $a, b \in [M]$, $\text{Pr}[h(i)=a \land h(j)=b]=\frac{1}{M^2}$
\end{definition}

It works because in previous calculation, we only care about pairs. We defined $X_i=1$ iff $h(i)$ is small than a threshold, then we computed $\text{var}[\Sigma X_i] = E[(\Sigma X_i)^2] - E[(\Sigma X_i)^2] = E[X_1X_1 + X_1X_2 + \ldots]- E[(\Sigma X_i)^2]$. Notice that $E[X_iX_j]$ is the same for fully random $h$ and pairwise independent $h$.

\begin{example}
[Construct a pairwise independent hash]
Assume $M$ is a prime number (if not, we can always pick a larger $M$ that is a prime number). We pick $p, q \in \{0, 1, 2, \ldots M-1 \}$ and the hash function $h(i) = pi+q \mod M$. In this construction we only need $O(\log M) = O(\log n)$ space (to store $p, q, M$).
\end{example}

\begin{proof}
$h(i)=a, h(j)=b$ is equivalent to $pi+q \equiv a, pj+q \equiv b$. So $p(i-j) \equiv a-b$ and $p \equiv (a-b)(i-j)^{-1}, q \equiv a - pi$. Since $M$ is a prime number, the unique inverse implies that there is only one pair $(p, q)$ satisfies it. And the probability that pair is chosen is exactly $\frac{1}{M^2}$.
\end{proof}

