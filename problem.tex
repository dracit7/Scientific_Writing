\section{The Cache Coherency Problem}\label{sec:problem}

Modern processors cache is usually divided into multiple layers, namely L1, L2, and so on, to form a hierarchical system (See~\autoref{fig:hierarchy}).
The upper-level cache L1, sometimes L2 too, is typically implemented inside of each processor core, while lower-level caches are shared among cores.
While effectively reducing the bus traffic~\cite{archibald:ccpeuams,owicki:etposcc}, such design incurs the risk of cache inconsistency, which means two cores reading from the same memory address may receive different cached values.
Such a cache coherency problem, if not properly handled, can be a great threat to the system's correctness.

\begin{figure}
    \centering
    % \includegraphics[width=0.50\textwidth]{cache.pdf}
    \caption{A typical cache hierarchy diagram.}
    \label{fig:hierarchy}
\end{figure}

\subsection{Problem description}

In multicore systems, it is common for multiple separate cores to operate on the same memory unit.
Because of local cache's existance, each core maintains a copy of that memory unit, in form of a \emph{cache block}, in their own cache.
If one of these cores updates its cache block's value by writing to the corresponding memory address, an inconsistency among memory unit copies arises.
Without further synchronization operations, other cores will get outdated and incorrect values when they read from that memory address, causing a cache coherency problem.
This problem exists in both write-through and write-back caches.

\subsection{Categories of Solutions}

The cache coherency problem can be addressed by coordinating per-processor caches' read and write operations.
For instance, when a processor $P_0$ updates a cached memory location $M$'s value from $V_0$ to $V_1$ without informing others, if another processor $P_1$ has cached the same location before, there will be an inconsistent state:
when reading from $M$ after this update, $P_0$ gets the correct value $V_1$, but $P_1$ gets the outdated value $V_0$.
However, if $P_0$ informs $P_1$ to invalidate the cacheline containing $M$ and write-through $V_1$ to the memory, $P_1$'s read will trigger a cache miss, making it load the correct value $V_1$ from memory.
The aggregation of such coordination and rules are called \emph{cache coherency protocols}, whose implementation can be roughly categorized into three classes:
snooping, directory-based, and software approaches.

Snooping or snoopy coherency techniques~\cite{tang:csdittcms,goodman:ucmtrpt,katz:iaccp,em:dragon,thacker:firefly} are the most popular implementation in bus-sharing multiprocessor systems.
In these approaches, each local cache has a \emph{snoopy cache controller} which monitors transactions between the memory and other caches and updates the cache state accordingly~\cite{owicki:etposcc}.
Such techniques leverage system bus attributes (e.g., transaction atomicity) to ensure the simplicity of their designation.
While being efficient in common SMP systems, they face scalability issues when the bus bandwidth is not sufficient for serving transactions produced by numerous processors.

Directory-based techniques~\cite{agarwal:aeodsfcc,tang:csdittcms,patel:aomwpcm} use a directory to keep track of all cache blocks' status, including the block state and the set of processors that are sharing the block.
Compared with snooping approaches, directory-based techniques allow more fine-grained transaction monitoring instead of broadcasting each transaction to all processors connected to the bus.
These approaches achieve better scalability by reducing the bus traffic, but inevitably incur extra overhead for ensuring the atomicity and ordering of transaction signals.

Software-based techniques~\cite{smith:cccwssauoti,cytron:amopc} trade software complexity for hardware complexity.
They are highly extensible and configurable, requiring minimal hardware support.
Nevertheless, software-based solutions feel hard to achieve comparable performance to hardware-based competitors~\cite{owicki:etposcc}.
